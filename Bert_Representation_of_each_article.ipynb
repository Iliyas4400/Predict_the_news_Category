{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained BERT Base model to create vector for each text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FKWD9SiZCNed"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wOtG4cf0qVAZ"
   },
   "outputs": [],
   "source": [
    "#all imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import  tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQQWS0ex44z6",
    "outputId": "9fa6f215-2870-416f-f423-42f1cd554dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 9.8MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "pip install  sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTWRqbrBqVAu"
   },
   "source": [
    "<pre><font size=6> Preprocessing</font></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW3LCXxjt-cr",
    "outputId": "8ad5d279-a9b2-4694-fbfe-bbeee986a66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QzjyMXPls78J"
   },
   "outputs": [],
   "source": [
    "X_train_p,y_train = pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/train_data_bert.pkl\", 'rb')) \n",
    "X_test_p = pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/test_data_bert.pkl\",'rb'))\n",
    "X_cv_p,y_cv =  pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/cv_data_bert.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBtqNGN9qVBM"
   },
   "source": [
    "<pre><font size=6>Creating BERT Model</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i8xd2HejqVBN"
   },
   "outputs": [],
   "source": [
    "## Loading the Pretrained Model from tensorflow HUB\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# maximum length of a seq in the data we have, for now i am making it as 55. You can change this\n",
    "max_seq_length = 512\n",
    "\n",
    "#BERT takes 3 inputs\n",
    "\n",
    "#this is input words. Sequence of words represented as integers\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "\n",
    "#mask vector if you are padding anything\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "\n",
    "#segment vectors. If you are giving only one sentence for the classification, total seg vector is 0. \n",
    "#If you are giving two sentenced with [sep] token separated, first seq segment vectors are zeros and \n",
    "#second seq segment vector are 1's\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7nCl7xzwxPuk"
   },
   "outputs": [],
   "source": [
    "#bert layer \n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "#Bert model\n",
    "#We are using only pooled output not sequence out. \n",
    "#If you want to know about those, please read https://www.kaggle.com/questions-and-answers/86510\n",
    "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)\n",
    "#bert_model = Model(inputs=[input_word_ids],outputs=pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQJsjg6fqVBQ",
    "outputId": "c8371538-411f-4e7b-f376-68a5ee19f0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3z0OMA5qVBS",
    "outputId": "b6f1dfe4-8c7c-495e-9f30-1799fb82d33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'keras_layer')>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ewv4hFCsqVBU"
   },
   "source": [
    "<pre><font size=6>Tokenization</font></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3aavTjce5L6X"
   },
   "outputs": [],
   "source": [
    "#getting Vocab file\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V8WEs6gfk58E"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/ Deep learning assignments/ NLP with Transfer Leraning')\n",
    "\n",
    "from tokenization import FullTokenizer\n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Iep3cQFgw_p5"
   },
   "outputs": [],
   "source": [
    "def tokenize(X):\n",
    "    X_tokens = np.array([0]*max_seq_length)\n",
    "    X_mask = np.array([0]*max_seq_length)\n",
    "    X_segment = np.array([0]*max_seq_length)  \n",
    "\n",
    "    for i in tqdm(range(len(X))):\n",
    "      tokens = tokenizer.tokenize(X[i]) #if the article length is more than 512 take first and last half portions\n",
    "      if len(tokens)>=(max_seq_length-2):\n",
    "        tokens1=tokens[0:(max_seq_length-2)//2]\n",
    "        tokens2=tokens[(max_seq_length-2)//2:max_seq_length-2]\n",
    "        tokens=np.append(tokens1,tokens2)\n",
    "      tokens=['[CLS]',*tokens,'[SEP]']\n",
    "      t=len(tokens)\n",
    "    \n",
    "      if len(tokens)<max_seq_length:\n",
    "        for i in range(len(tokens),max_seq_length):\n",
    "          tokens.append('[PAD]')\n",
    "\n",
    "      a=np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "      X_tokens=np.vstack((X_tokens,a))\n",
    "\n",
    "      b=np.array([1]*t+[0]*(max_seq_length-t))\n",
    "      X_mask=np.vstack((X_mask,b))\n",
    "\n",
    "      b=np.array([0]*max_seq_length)\n",
    "      X_segment = np.vstack((X_segment,b))\n",
    "     \n",
    "    X_tokens=X_tokens[1:,]\n",
    "    X_mask=X_mask[1:,]\n",
    "    X_segment=X_segment[1:,]\n",
    "    return (X_tokens,X_mask,X_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3qM4WXIU6cI",
    "outputId": "c93e07a9-ddf0-45d8-8687-e7f24672e551"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6712/6712 [01:30<00:00, 74.42it/s]\n",
      "100%|██████████| 2748/2748 [00:15<00:00, 179.36it/s]\n",
      "100%|██████████| 916/916 [00:02<00:00, 327.56it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens,X_train_mask,X_train_segment =  tokenize(X_train_p)\n",
    "X_test_tokens,X_test_mask,X_test_segment =  tokenize(X_test_p)\n",
    "X_cv_tokens,X_cv_mask,X_cv_segment =  tokenize(X_cv_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAF2Pj420n5p",
    "outputId": "da896dbe-5289-4151-e664-c5780af2fbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_tokens :  (6712, 512)\n",
      "shape of X_train_mask :  (6712, 512)\n",
      "shape of X_train_segment :  (6712, 512)\n",
      "==================================================\n",
      "shape of X_test_tokens :  (2748, 512)\n",
      "shape of X_test_mask :  (2748, 512)\n",
      "shape of X_test_segment :  (2748, 512)\n",
      "==================================================\n",
      "shape of X_cv_tokens :  (916, 512)\n",
      "shape of X_cv_mask :  (916, 512)\n",
      "shape of X_cv_segment :  (916, 512)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train_tokens : ',X_train_tokens.shape)\n",
    "print('shape of X_train_mask : ',X_train_mask.shape)\n",
    "print('shape of X_train_segment : ',X_train_segment.shape)\n",
    "\n",
    "print('='*50)\n",
    "\n",
    "print('shape of X_test_tokens : ',X_test_tokens.shape)\n",
    "print('shape of X_test_mask : ',X_test_mask.shape)\n",
    "print('shape of X_test_segment : ',X_test_segment.shape)\n",
    "\n",
    "print('='*50)\n",
    "\n",
    "print('shape of X_cv_tokens : ',X_cv_tokens.shape)\n",
    "print('shape of X_cv_mask : ',X_cv_mask.shape)\n",
    "print('shape of X_cv_segment : ',X_cv_segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PptNNePp00DK"
   },
   "outputs": [],
   "source": [
    " \n",
    "pickle.dump((X_train_tokens,X_train_mask,X_train_segment, y_train),open('/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/train_bert.pkl','wb'))\n",
    "pickle.dump((X_train_tokens,X_train_mask,X_train_segment),open('/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/test_bert.pkl','wb'))\n",
    "pickle.dump((X_train_tokens,X_train_mask,X_train_segment, y_cv),open('/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/cv_bert.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1GRrhV2Z07jI"
   },
   "outputs": [],
   "source": [
    "X_train_tokens,X_train_mask,X_train_segment,y_train = pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/train_bert.pkl\", 'rb')) \n",
    "X_train_tokens,X_train_mask,X_train_segment = pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/test_bert.pkl\",'rb'))\n",
    "X_train_tokens,X_train_mask,X_train_segment,y_cv =  pickle.load(open(\"/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/cv_bert.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVtux9qb2J7f",
    "outputId": "3568d23b-8561-445d-ccd1-8b2f571c5505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_tokens :  (6712, 512)\n",
      "shape of X_train_mask :  (6712, 512)\n",
      "shape of X_train_segment :  (6712, 512)\n",
      "==================================================\n",
      "shape of X_test_tokens :  (2748, 512)\n",
      "shape of X_test_mask :  (2748, 512)\n",
      "shape of X_test_segment :  (2748, 512)\n",
      "==================================================\n",
      "shape of X_cv_tokens :  (916, 512)\n",
      "shape of X_cv_mask :  (916, 512)\n",
      "shape of X_cv_segment :  (916, 512)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train_tokens : ',X_train_tokens.shape)\n",
    "print('shape of X_train_mask : ',X_train_mask.shape)\n",
    "print('shape of X_train_segment : ',X_train_segment.shape)\n",
    "\n",
    "print('='*50)\n",
    "\n",
    "print('shape of X_test_tokens : ',X_test_tokens.shape)\n",
    "print('shape of X_test_mask : ',X_test_mask.shape)\n",
    "print('shape of X_test_segment : ',X_test_segment.shape)\n",
    "\n",
    "print('='*50)\n",
    "\n",
    "print('shape of X_cv_tokens : ',X_cv_tokens.shape)\n",
    "print('shape of X_cv_mask : ',X_cv_mask.shape)\n",
    "print('shape of X_cv_segment : ',X_cv_segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nq1w_UDBRU0y",
    "outputId": "0acc5b8e-265e-4989-94e8-73c52908087b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token array :  [  101  2250  9834  1998 29536  2850 14876  2638  2801  3613  2000  8292\n",
      "  3207  2598  2000 17975 10147  2080  1010  2007  3006  3745  6409  1999\n",
      "  2048  5486  7728  2005  2119  3316  1012  1006 12927  1007  2096  1996\n",
      "  6393  1997 17975 10147  2080  2003  7058  5310  2918 13134  2038  9784\n",
      "  1010  2009  2003  2145  4089  2041 19498  2075  2250  9834  1998 29536\n",
      "  2850 14876  2638  2801 17975 10147  2080  2038  4227  3745  1999  3923\n",
      "  2752  2750  2250  9834  1998 29536  2850 14876  2638  2801  1005  1055\n",
      "  3579  2006  2122  4655   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "Mask array  :  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Segment array :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Token array : ',X_train_tokens[0])\n",
    "print('Mask array  : ',X_train_mask[0])\n",
    "print('Segment array : ',X_train_segment[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEj-Eua5qVBx"
   },
   "source": [
    "<pre><font size=6> Getting Embeddings from BERT Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwOVgQFDqVBy",
    "outputId": "cadcbe78-bf14-4ac7-a25a-996f860ba5ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 512) dtype=int32 (created by layer 'input_word_ids')>,\n",
       " <KerasTensor: shape=(None, 512) dtype=int32 (created by layer 'input_mask')>,\n",
       " <KerasTensor: shape=(None, 512) dtype=int32 (created by layer 'segment_ids')>]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcpkQq1OqVB0",
    "outputId": "ce920948-d30f-4938-9d6a-2a63cd0146af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'keras_layer')>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IxdIlOIBlm7j"
   },
   "outputs": [],
   "source": [
    "# get the train output, BERT model will give one output so save in\n",
    "# X_train_pooled_output\n",
    "X_train_pooled_output=bert_model.predict([X_train_tokens,X_train_mask,X_train_segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "sPpAMoWeWHeO"
   },
   "outputs": [],
   "source": [
    "X_cv_pooled_output=bert_model.predict([X_cv_tokens,X_cv_mask,X_cv_segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yZT11BCol4gL"
   },
   "outputs": [],
   "source": [
    "# get the test output, BERT model will give one output so save in\n",
    "# X_test_pooled_output\n",
    "X_test_pooled_output=bert_model.predict([X_test_tokens,X_test_mask,X_test_segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DL6JVojfqVB8"
   },
   "outputs": [],
   "source": [
    "##save all your results to disk so that, no need to run all again. \n",
    "pickle.dump((X_train_pooled_output, X_test_pooled_output,X_cv_pooled_output),open('/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/final_bert_output.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oSQcBdROqVB9"
   },
   "outputs": [],
   "source": [
    "X_train_pooled_output, X_test_pooled_output,X_cv_pooled_output= pickle.load(open('/content/drive/MyDrive/ Competitions/ Predict_the_news_category/ data/final_bert_output.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fze5l62mbu2c",
    "outputId": "82c9848b-4982-49a6-d854-f0081904d717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6712, 768)\n",
      "(2748, 768)\n",
      "(916, 768)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pooled_output.shape)\n",
    "print(X_test_pooled_output.shape)\n",
    "print(X_cv_pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "x0bxHTeOntSh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Bert1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
